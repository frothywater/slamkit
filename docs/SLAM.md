# Slam
The official code for ["_Slamming_: Training a Speech Language Model on One GPU in a Day"](https://arxiv.org/abs/2502.15814).

<p align="center">
    üåê <a href="https://pages.cs.huji.ac.il/adiyoss-lab/slamming/" target="_blank">Project</a> | üìÉ <a href="https://arxiv.org/abs/2502.15814" target="_blank">Paper</a> | ü§ó <a href="https://huggingface.co/collections/slprl/slam-67b58a61b57083505c8876b2" target="_blank">Models & Datasets</a><br>
</p>


![https://pages.cs.huji.ac.il/adiyoss-lab/slamming/](../media/slam_web.png)


## Results
We provide some results for our pre-trained models, compared to other SLMs.

| Model                                     | GPUs    | Params | Num Tokens    | sBLIMP ‚Üë  | sStoryCloze ‚Üë | tStoryCloze ‚Üë | GenPPL ‚Üì | Auto-BLEU ‚Üì |
|-------------------------------------------|---------|--------|---------------|-----------|---------------|---------------|----------|-------------|
| **Speech only pre-training**              |         |        |               |           |               |               |          |             |
| GSLM                                      | 8√óV100  | 100M   | 1B            | 54.2      | 53.3          | 66.6          | ‚Äî        | ‚Äî           |
| SyllableLM                                | 4√óA40   | 300M   | 16B           | 63.7      | ‚Äî             | 75.4          | ‚Äî        | ‚Äî           |
| TWIST-350M                                | 8√óV100  | 305M   | 10.8B         | 56.2      | ‚Äî             | ‚Äî             | 137.3    | 3.46        |
| TWIST-1.3B                                | 32√óV100 | 1B     | 10.8B         | 57.0      | 52.4          | 70.6          | 131.8    | 3.20        |
| TWIST-7B                                  | 32√óV100 | 7B     | 36B           | 59.0      | 55.3          | 74.1          | 93.74    | 3.06        |
| TWIST-13B                                 | 32√óV100 | 13B    | 36B           | 59.2      | 55.4          | 76.4          | ‚Äî        | ‚Äî           |
| Scaled Optimal                            | ‚Äî       | 823M   | 82B           | **61.3**  | 56.7          | 78.0          | ‚Äî        | ‚Äî           |
| Moshi                                     | ?√óH100  | 7B     | ?             | 58.9      | **58.7**      | **81.8**      | ‚Äî        | ‚Äî           |
| SpiritLM                                  | 64√óA100 | 7B     | 100B          | 58.0      | 54.8          | 72.9          | ‚Äî        | ‚Äî           |
| **With text / preference optimization**   |         |        |               |           |               |               |          |             |
| Scaling Interleaving                      | ‚Äî       | 9B     | ~1T           | ‚Äî         | **62.4**      | 82.9          | ‚Äî        | ‚Äî           |
| Moshi                                     | ?√óH100  | 7B     | ~720B         | 58.8      | 60.8          | 83.0          | ‚Äî        | ‚Äî           |
| SpiritLM                                  | 64√óA100 | 7B     | 100B          | 58.3      | 61.0          | 82.9          | ‚Äî        | ‚Äî           |
| AlignSLM-1.3B                             | 64√óA100 | 1B     | 10.8B + ~158B | 59.8      | 55.0          | 80.0          | ‚Äî        | ‚Äî           |
| AlignSLM-7B                               | 64√óA100 | 7B     | 36B + ~158B   | **62.3**  | 61.1          | **86.8**      | ‚Äî        | ‚Äî           |
| **Ours (_Slam_)**                         |         |        |               |           |               |               |          |             |
| _Slam_ (-DPO)                             | 2√óA100  | 358M   | 16.7B         | 58.53     | 58.15         | 80.71         | 67.3     | 3.25        |
| _Slam_                                    | 1√óA5000 | 358M   | 1.4B + 5M     | 58.86     | 58.04         | 82.04         | 62.8     | 3.88        |
| _Slam_ (scaled)                           | 2√óA100  | 358M   | 16.7B + 9M    | **61.11** | **61.30**     | **84.18**     | **46.6** | 3.75        |


## Citation
If you use this work, please cite our paper:
```bibtex
@misc{maimon2025slamming,
      title={Slamming: Training a Speech Language Model on One GPU in a Day}, 
      author={Gallil Maimon and Avishai Elmakies and Yossi Adi},
      year={2025},
      eprint={2502.15814},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.15814}, 
}
```